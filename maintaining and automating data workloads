Diagnostic questions
Your score: 60% Passing score: 75%
Unfortunately, you need at least a 75% to pass this assessment. Not to worry though, review your answers and try again.
close
1.

Cymbal Retail processes streaming data on Dataflow with Pub/Sub as a source. You need to plan for disaster recovery and protect against zonal failures. What should you do?

Enable vertical autoscaling.

Take Dataflow snapshots periodically.

Enable Dataflow shuffle.
close
Create Dataflow jobs from templates.
Incorrect. Creating jobs from templates makes it easy to start new jobs by changing parameter values. However, since templates do not retain any state, they are not useful for disaster recovery.
check
2.

You have a Dataflow pipeline in production. For certain data, the system seems to be stuck longer than usual. This is causing delays in the pipeline execution. You want to reliably and proactively track and resolve such issues. What should you do?

Review the Dataflow logs regularly.
check
Set up alerts on Cloud Monitoring based on system lag.

Review the Cloud Monitoring dashboard regularly.

Set up alerts with Cloud Functions code that reviews the audit logs regularly.
Correct. Setting up alerts proactively notifies users about issues or metrics that need to be tracked.
close
3.

A colleague at Cymbal Retail asks you about the configuration of Dataproc autoscaling for a project. What would be the Google-recommended situation when you should enable autoscaling?

When you want to scale out single-job clusters.

When you want to scale on-cluster Hadoop Distributed File System (HDFS).
close
When there are different size workloads on the cluster.

When you want to down-scale idle clusters to minimum size.
Incorrect. Running different size workloads on the same cluster can cause interference. Long-running jobs might interfere with and delay the downscaling of smaller jobs.
check
4.

You need to create repeatable data processing tasks by using Cloud Composer. You need to follow best practices and recommended approaches. What should you do?

Combine multiple functionalities in a single task execution.

Update data with INSERT statements during the task run.

Use current time with the now() function for computation.
check
Write each task to be responsible for one operation.
Correct. To run repeatable tasks, it is recommended to use atomic tasks that have a single responsibility. Many of these tasks can be combined in sequence to achieve a desired end result.
check
5.

Multiple analysts need to prepare reports on Monday mornings due to which there is heavy utilization of BigQuery. You want to take a cost-effective approach to managing this demand. What should you do?
check
Use Flex Slots.

Use BigQuery Enterprise Plus edition with a three-year commitment.

Use on-demand pricing.

Use BigQuery Enterprise edition with a one-year commitment.
Correct. Flex Slots let you reserve BigQuery slots for short durations.
close
6.

You have a team of data analysts that run queries interactively on BigQuery during work hours. You also have thousands of report generation queries that run simultaneously. You often see an error: Exceeded rate limits: too many concurrent queries for this project_and_region. How would you resolve this issue?

Run the report generation queries in batch mode.

Run all queries in interactive mode.

Create a view to run the queries.
close
Create a yearly reservation of BigQuery slots.
Incorrect. Reserving BigQuery slots will not avoid the error because concurrent query limits still apply.
close
7.

You need to design a Dataproc cluster to run multiple small jobs. Many jobs (but not all) are of high priority. What should you do?

Use ephemeral clusters.

Reuse the same cluster to run all jobs in parallel.

Use cluster autoscaling.
close
Reuse the same cluster and run each job in sequence.
Incorrect. Because many jobs are high priority, running them in sequence would not meet the business requirements.
check
8.

You are running a Dataflow pipeline in production. The input data for this pipeline is occasionally inconsistent. Separately from processing the valid data, you want to efficiently capture the erroneous input data for analysis. What should you do?
check
Create a side output for the erroneous data.

Check for the erroneous data in the logs.

Re-read the input data and create separate outputs for valid and erroneous data.

Read the data once, and split it into two pipelines, one to output valid data and another to output erroneous data.
Correct. Using side outputs can collect the erroneous data efficiently and is a recommended approach.
check
9.

When running Dataflow jobs, you see this error in the logs: "A hot key HOT_KEY_NAME was detected inâ€¦". You need to resolve this issue and make the workload performant. What should you do?
check
Ensure that your data is evenly distributed.

Add more compute instances for processing.

Increase the data with the hot key.

Disable Dataflow shuffle.
Correct. The Dataflow transformations are more performant with an evenly distributed key.
check
10.

You run a Cloud SQL instance for a business that requires that the database is accessible for transactions. You need to ensure minimal downtime for database transactions. What should you do?

Configure backups.

Configure backups and increase the number of backups.
check
Configure high availability.

Configure replication.
Correct. Configuring high availability on Cloud SQL will automatically switch to the secondary instance when the primary instance goes down, thus reducing downtime for the database's users.
